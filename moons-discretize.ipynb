{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89dfd5-7692-45bc-b126-862b3cb0c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets as skd\n",
    "import sklearn.preprocessing as skpp\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47025183-b6df-4c37-abb4-545e4343b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.deprecation import _deprecate_Xt_in_inverse_transform\n",
    "from sklearn.utils.validation import (\n",
    "    _check_feature_names_in,\n",
    "    _check_sample_weight,\n",
    "    check_array,\n",
    "    check_is_fitted,\n",
    ")\n",
    "class KBinsDiscretizerSampler(KBinsDiscretizer):\n",
    "    def inverse_transform_sample(self, X=None, random_state=None):\n",
    "        rng = check_random_state(random_state)\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        if \"onehot\" in self.encode:\n",
    "            X = self._encoder.inverse_transform(X)\n",
    "\n",
    "        Xinv = check_array(X, copy=True, dtype=(np.float64, np.float32))\n",
    "        n_features = self.n_bins_.shape[0]\n",
    "        if Xinv.shape[1] != n_features:\n",
    "            raise ValueError(\n",
    "                \"Incorrect number of features. Expecting {}, received {}.\".format(\n",
    "                    n_features, Xinv.shape[1]\n",
    "                )\n",
    "            )\n",
    "        n = X.shape[0]\n",
    "        for jj in range(n_features):\n",
    "            jitter = rng.uniform(0., 1., size=n)\n",
    "            bin_edges = self.bin_edges_[jj]\n",
    "            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\n",
    "            bin_lefts = bin_edges[1:][(Xinv[:, jj]).astype(np.int64)]\n",
    "            bin_rights = bin_edges[:-1][(Xinv[:, jj]).astype(np.int64)]\n",
    "            Xinv[:, jj] = bin_lefts * jitter + bin_rights * (1 - jitter)\n",
    "\n",
    "        return Xinv\n",
    "\n",
    "def top_p_sampling(n_bins, probs, rng, top_p):\n",
    "    probs = probs.ravel()\n",
    "    # https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    sorted_probs = probs[sorted_indices]\n",
    "    cumulative_probs = np.cumsum(sorted_probs)\n",
    "\n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Shift the indices to the right to keep also the first token above the threshold\n",
    "    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1]\n",
    "    sorted_indices_to_remove[0] = False\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    probs[indices_to_remove] = 0.\n",
    "    probs = probs / np.sum(probs)\n",
    "\n",
    "    chosen = np.array(rng.choice(n_bins, p=probs))\n",
    "    return chosen\n",
    "\n",
    "class MaskingTreesModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_bins=5,\n",
    "        duplicate_K=50,\n",
    "        top_p=0.8,\n",
    "        random_state = None,\n",
    "    ):\n",
    "        self.n_bins = n_bins\n",
    "        self.duplicate_K = duplicate_K\n",
    "        self.top_p = top_p\n",
    "        self.random_state = random_state\n",
    "\n",
    "        assert 2 <= n_bins\n",
    "        assert 1 <= duplicate_K\n",
    "        assert 0 < top_p <= 1\n",
    "    \n",
    "        self.xgbers_ = None\n",
    "        self.quantize_cols_ = None\n",
    "        self.quantizers_ = None\n",
    "        self.X_ = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        quantize_cols='all',\n",
    "    ):\n",
    "        # TODO - handle categorical columns\n",
    "        # TODO - xgboost iterator - generate batches on the fly\n",
    "        # TODO - xgboost kwargs\n",
    "        # TODO - sample_weight from OADM formula\n",
    "        # TODO - KDITransformer\n",
    "        # TODO - nucleus sampling\n",
    "        rng = check_random_state(self.random_state)\n",
    "        n_samples, n_dims = X.shape\n",
    "        if isinstance(quantize_cols, list):\n",
    "            assert len(quantize_cols) == n_dims\n",
    "        elif quantize_cols == 'auto':\n",
    "            assert isinstance(X, pd.DataFrame)  # uses \n",
    "        elif quantize_cols == 'none':\n",
    "            self.quantize_cols_ = [False] * n_dims\n",
    "        elif quantize_cols == 'all':\n",
    "            self.quantize_cols_ = [True] * n_dims\n",
    "        else:\n",
    "            raise ValueError(f'unexpected quantize_cols: {quantize_cols}')\n",
    "        self.X_ = X.copy()\n",
    "        self.quantizers_ = []\n",
    "        \n",
    "        for d in range(n_dims):\n",
    "            curq = KBinsDiscretizerSampler(\n",
    "                n_bins=self.n_bins, encode='ordinal', strategy='quantile')\n",
    "            curq.fit(X[~np.isnan(X[:, d]), d:d+1])\n",
    "            self.quantizers_.append(curq)\n",
    "\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        for dupix in range(self.duplicate_K):\n",
    "            mask_ixs = np.repeat(np.arange(n_dims)[np.newaxis, :], n_samples, axis=0)\n",
    "            mask_ixs = np.apply_along_axis(rng.permutation, axis=1, arr=mask_ixs) # n_samples, n_dims\n",
    "            for n in range(n_samples):\n",
    "                fuller_X = X[n, :]\n",
    "                for d in range(n_dims):\n",
    "                    victim_ix = mask_ixs[n, d]\n",
    "                    if fuller_X[victim_ix] != np.nan:\n",
    "                        emptier_X = fuller_X.copy()\n",
    "                        emptier_X[mask_ixs[n, d]] = np.nan\n",
    "                        X_train.append(emptier_X.reshape(1, -1))\n",
    "                        Y_train.append(fuller_X.reshape(1, -1))\n",
    "                        fuller_X = emptier_X\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        Y_train = np.concatenate(Y_train, axis=0)\n",
    "        self.trees_ = []\n",
    "        for d in range(n_dims):\n",
    "            xgber = xgb.XGBClassifier(tree_method=\"hist\") # TODO: early_stopping_rounds=2)\n",
    "            train_ixs = ~np.isnan(Y_train[:, d])\n",
    "            curY_train = self.quantizers_[d].transform(Y_train[train_ixs, d:d+1])        \n",
    "            curX_train = X_train[train_ixs, :] \n",
    "            xgber.fit(curX_train, curY_train)  # TODO: sample_weight\n",
    "            self.trees_.append(xgber)\n",
    "        return self\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        n_samples=1,\n",
    "    ):\n",
    "        n_samples, n_dims = self.X_.shape\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        X = np.full(fill_value=np.nan, shape=(n_samples, n_dims))\n",
    "        unmask_ixs = np.repeat(np.arange(n_dims)[np.newaxis, :], n_samples, axis=0)  # (n_samples, n_dims)\n",
    "        unmask_ixs = np.apply_along_axis(rng.permutation, axis=1, arr=unmask_ixs) # (n_samples, n_dims)\n",
    "        for n in range(n_samples):\n",
    "            for dix in range(n_dims):\n",
    "                unmask_ix = unmask_ixs[n, dix]\n",
    "                pred_probas = self.trees_[unmask_ix].predict_proba(X[[n], :])\n",
    "                pred_quant = top_p_sampling(self.n_bins, pred_probas, rng, self.top_p)\n",
    "                pred_val = self.quantizers_[unmask_ix].inverse_transform_sample(pred_quant.reshape(1, 1))\n",
    "                X[n, unmask_ix] = pred_val.item()\n",
    "        return X\n",
    "\n",
    "    def impute(\n",
    "        self,\n",
    "        k=1,\n",
    "    ):\n",
    "        (n_samples, n_dims) = self.X_.shape\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        imputedX = np.repeat(self.X_[np.newaxis, :, :], repeats=k, axis=0) # (k, n_samples, n_dims)           \n",
    "        for n in range(n_samples):\n",
    "            to_unmask = np.where(np.isnan(self.X_[n, :]))[0] # (n_to_unmask,)\n",
    "            unmask_ixs = np.repeat(to_unmask[np.newaxis, :], k, axis=0)  # (k, n_to_unmask)\n",
    "            unmask_ixs = np.apply_along_axis(rng.permutation, axis=1, arr=unmask_ixs) # (k, n_to_unmask)\n",
    "            n_to_unmask = unmask_ixs.shape[1]\n",
    "            for kix in range(k):\n",
    "                for dix in range(n_to_unmask):\n",
    "                    unmask_ix = unmask_ixs[kix, dix]\n",
    "                    pred_probas = self.trees_[unmask_ix].predict_proba(imputedX[kix,[n], :])\n",
    "                    pred_quant = top_p_sampling(self.n_bins, pred_probas, rng, self.top_p)\n",
    "                    pred_val = self.quantizers_[unmask_ix].inverse_transform_sample(pred_quant.reshape(1, 1))\n",
    "                    imputedX[kix, n, unmask_ix] = pred_val.item()\n",
    "        return imputedX\n",
    "\n",
    "rix = 0\n",
    "rng = check_random_state(rix)\n",
    "n_upper = 100\n",
    "n_lower = 100\n",
    "n = n_upper + n_lower\n",
    "data, labels = skd.make_moons(\n",
    "    (n_upper, n_lower), shuffle=False, noise=0.1, random_state=rix)\n",
    "data4impute = data.copy()\n",
    "data4impute[:, 1] = np.nan\n",
    "X=np.concatenate([data, data4impute], axis=0)\n",
    "\n",
    "model = MaskingTreesModel(n_bins=20)\n",
    "model.fit(X)\n",
    "data_fake = model.generate(n_samples=200);\n",
    "\n",
    "\n",
    "\n",
    "nimp = 1 # number of imputations needed\n",
    "data_impute = model.impute(k=nimp)[0, :, :]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(7, 5));\n",
    "axes[0, 0].scatter(data[:, 0], data[:, 1]);\n",
    "axes[0, 0].set_title('original');\n",
    "axes[0, 1].scatter(data_fake[:, 0], data_fake[:, 1]);\n",
    "axes[0, 1].set_title('generated');\n",
    "axes[1, 0].scatter(data_impute[200:, 0], data_impute[200:, 1]);\n",
    "axes[1, 0].set_title('imputed');\n",
    "\"\"\"\n",
    "axes[1, 1].scatter(data_impute[200:, 0], data_impute[200:, 1]);\n",
    "axes[1, 1].set_title('imputed - repainted');\n",
    "\"\"\"\n",
    "plt.tight_layout();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5405d-0f05-4b93-b227-43f739c9a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(size=(5,))\n",
    "a = np.exp(a) / np.sum(np.exp(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144532d-f9c6-4c35-80a4-392c5e85b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indices = np.argsort(a)[::-1]\n",
    "sort_probs = a[sort_indices]\n",
    "print(sort_probs)\n",
    "cumsum_probs = np.cumsum(sort_probs)\n",
    "print(cumsum_probs)\n",
    "unnorm_probs = np.diff(np.minimum(cumsum_probs, 0.8), prepend=0.)\n",
    "unnorm_probs = unnorm_probs[np.argsort(sort_indices)]  # undo the sort\n",
    "norm_probs = unnorm_probs / np.sum(unnorm_probs)\n",
    "\n",
    "print(norm_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8cb4ce-0344-4b75-b35a-29d470918b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c48fb-2718-4121-befb-d5e08e98ef17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
