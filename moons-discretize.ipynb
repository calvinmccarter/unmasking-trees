{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89dfd5-7692-45bc-b126-862b3cb0c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets as skd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47025183-b6df-4c37-abb4-545e4343b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import KBinsDiscretizer, LabelEncoder\n",
    "from sklearn.utils.deprecation import _deprecate_Xt_in_inverse_transform\n",
    "from sklearn.utils.validation import (\n",
    "    _check_feature_names_in,\n",
    "    _check_sample_weight,\n",
    "    check_array,\n",
    "    check_is_fitted,\n",
    ")\n",
    "class KBinsDiscretizerSampler(KBinsDiscretizer):\n",
    "    def inverse_transform_sample(self, X=None, random_state=None):\n",
    "        rng = check_random_state(random_state)\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        if \"onehot\" in self.encode:\n",
    "            X = self._encoder.inverse_transform(X)\n",
    "\n",
    "        Xinv = check_array(X, copy=True, dtype=(np.float64, np.float32))\n",
    "        n_features = self.n_bins_.shape[0]\n",
    "        if Xinv.shape[1] != n_features:\n",
    "            raise ValueError(\n",
    "                \"Incorrect number of features. Expecting {}, received {}.\".format(\n",
    "                    n_features, Xinv.shape[1]\n",
    "                )\n",
    "            )\n",
    "        n = X.shape[0]\n",
    "        for jj in range(n_features):\n",
    "            jitter = rng.uniform(0., 1., size=n)\n",
    "            bin_edges = self.bin_edges_[jj]\n",
    "            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\n",
    "            bin_lefts = bin_edges[1:][(Xinv[:, jj]).astype(np.int64)]\n",
    "            bin_rights = bin_edges[:-1][(Xinv[:, jj]).astype(np.int64)]\n",
    "            Xinv[:, jj] = bin_lefts * jitter + bin_rights * (1 - jitter)\n",
    "\n",
    "        return Xinv\n",
    "\n",
    "def top_p_sampling(n_bins, probs, rng, top_p):\n",
    "    \"\"\" This implements a modified version of nucleus sampling.\n",
    "    It discards probability mass beyond the boundary of the class straddles the top_p boundary,\n",
    "    but it does not discard the probability mass of this class below the boundary.\n",
    "    \"\"\"\n",
    "    probs = probs.ravel()  # currently assumes only one sample\n",
    "    sort_indices = np.argsort(probs)[::-1]\n",
    "    sort_probs = probs[sort_indices]\n",
    "    cumsum_probs = np.cumsum(sort_probs)\n",
    "    unnorm_probs = np.diff(np.minimum(cumsum_probs, top_p), prepend=0.)\n",
    "    unnorm_probs = unnorm_probs[np.argsort(sort_indices)]  # undo the sort\n",
    "    norm_probs = unnorm_probs / np.sum(unnorm_probs)\n",
    "    chosen = np.array(rng.choice(n_bins, p=norm_probs))\n",
    "    return chosen\n",
    "\n",
    "class MaskingTreesModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_bins=5,\n",
    "        duplicate_K=50,\n",
    "        top_p=0.9,\n",
    "        random_state = None,\n",
    "    ):\n",
    "        self.n_bins = n_bins\n",
    "        self.duplicate_K = duplicate_K\n",
    "        self.top_p = top_p\n",
    "        self.random_state = random_state\n",
    "\n",
    "        assert 2 <= n_bins\n",
    "        assert 1 <= duplicate_K\n",
    "        assert 0 < top_p <= 1\n",
    "    \n",
    "        self.xgbers_ = None\n",
    "        self.quantize_cols_ = None\n",
    "        self.quantizers_ = None\n",
    "        self.X_ = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        quantize_cols='all',\n",
    "    ):\n",
    "        # TODO - xgboost iterator - generate batches on the fly\n",
    "        # TODO - xgboost kwargs\n",
    "        # TODO - sample_weight from OADM formula\n",
    "        # TODO - KDITransformer\n",
    "        rng = check_random_state(self.random_state)\n",
    "        assert isinstance(X, np.ndarray)\n",
    "        n_samples, n_dims = X.shape\n",
    "        if isinstance(quantize_cols, list):\n",
    "            assert len(quantize_cols) == n_dims\n",
    "            self.quantize_cols_ = quantize_cols\n",
    "        elif quantize_cols == 'none':\n",
    "            self.quantize_cols_ = [False] * n_dims\n",
    "        elif quantize_cols == 'all':\n",
    "            self.quantize_cols_ = [True] * n_dims\n",
    "        else:\n",
    "            raise ValueError(f'unexpected quantize_cols: {quantize_cols}')\n",
    "\n",
    "        self.X_ = X.copy()\n",
    "\n",
    "        \n",
    "        self.quantizers_ = []       \n",
    "        for d in range(n_dims):\n",
    "            if self.quantize_cols_[d]:\n",
    "                curq = KBinsDiscretizerSampler(\n",
    "                    n_bins=self.n_bins, encode='ordinal', strategy='quantile')\n",
    "                curq.fit(X[~np.isnan(X[:, d]), d:d+1])\n",
    "            else:\n",
    "                curq = LabelEncoder()\n",
    "                curq.fit(X[~np.isnan(X[:, d]), d])\n",
    "            self.quantizers_.append(curq)\n",
    "\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        for dupix in range(self.duplicate_K):\n",
    "            mask_ixs = np.repeat(np.arange(n_dims)[np.newaxis, :], n_samples, axis=0)\n",
    "            mask_ixs = np.apply_along_axis(rng.permutation, axis=1, arr=mask_ixs) # n_samples, n_dims\n",
    "            for n in range(n_samples):\n",
    "                fuller_X = X[n, :]\n",
    "                for d in range(n_dims):\n",
    "                    victim_ix = mask_ixs[n, d]\n",
    "                    if fuller_X[victim_ix] != np.nan:\n",
    "                        emptier_X = fuller_X.copy()\n",
    "                        emptier_X[mask_ixs[n, d]] = np.nan\n",
    "                        X_train.append(emptier_X.reshape(1, -1))\n",
    "                        Y_train.append(fuller_X.reshape(1, -1))\n",
    "                        fuller_X = emptier_X\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        Y_train = np.concatenate(Y_train, axis=0)\n",
    "        self.trees_ = []\n",
    "        for d in range(n_dims):\n",
    "            xgber = xgb.XGBClassifier(tree_method=\"hist\") # TODO: early_stopping_rounds=2)\n",
    "            train_ixs = ~np.isnan(Y_train[:, d])\n",
    "            if self.quantize_cols_[d]:\n",
    "                curY_train = self.quantizers_[d].transform(Y_train[train_ixs, d:d+1])\n",
    "            else:\n",
    "                curY_train = self.quantizers_[d].transform(Y_train[train_ixs, d])\n",
    "            curX_train = X_train[train_ixs, :] \n",
    "            xgber.fit(curX_train, curY_train)  # TODO: sample_weight\n",
    "            self.trees_.append(xgber)\n",
    "        return self\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        n_samples=1,\n",
    "    ):\n",
    "        n_samples, n_dims = self.X_.shape\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        X = np.full(fill_value=np.nan, shape=(n_samples, n_dims))\n",
    "        unmask_ixs = np.repeat(np.arange(n_dims)[np.newaxis, :], n_samples, axis=0)  # (n_samples, n_dims)\n",
    "        unmask_ixs = np.apply_along_axis(rng.permutation, axis=1, arr=unmask_ixs) # (n_samples, n_dims)\n",
    "        for n in range(n_samples):\n",
    "            for dix in range(n_dims):\n",
    "                unmask_ix = unmask_ixs[n, dix]\n",
    "                pred_probas = self.trees_[unmask_ix].predict_proba(X[[n], :])\n",
    "                cur_quant = self.quantizers_[unmask_ix]\n",
    "                if self.quantize_cols_[unmask_ix]:\n",
    "                    pred_quant = top_p_sampling(cur_quant.n_bins_[0], pred_probas, rng, self.top_p)\n",
    "                    pred_val = cur_quant.inverse_transform_sample(pred_quant.reshape(1, 1))\n",
    "                else:\n",
    "                    pred_quant = top_p_sampling(len(cur_quant.classes_), pred_probas, rng, self.top_p)\n",
    "                    pred_val = cur_quant.inverse_transform(pred_quant.reshape(1,))\n",
    "                X[n, unmask_ix] = pred_val.item()\n",
    "        return X\n",
    "\n",
    "    def impute(\n",
    "        self,\n",
    "        k=1,\n",
    "    ):\n",
    "        (n_samples, n_dims) = self.X_.shape\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        imputedX = np.repeat(self.X_[np.newaxis, :, :], repeats=k, axis=0) # (k, n_samples, n_dims)           \n",
    "        for n in range(n_samples):\n",
    "            to_unmask = np.where(np.isnan(self.X_[n, :]))[0] # (n_to_unmask,)\n",
    "            unmask_ixs = np.repeat(to_unmask[np.newaxis, :], k, axis=0)  # (k, n_to_unmask)\n",
    "            unmask_ixs = np.apply_along_axis(rng.permutation, axis=1, arr=unmask_ixs) # (k, n_to_unmask)\n",
    "            n_to_unmask = unmask_ixs.shape[1]\n",
    "            for kix in range(k):\n",
    "                for dix in range(n_to_unmask):\n",
    "                    unmask_ix = unmask_ixs[kix, dix]\n",
    "                    pred_probas = self.trees_[unmask_ix].predict_proba(imputedX[kix,[n], :])\n",
    "                    cur_quant = self.quantizers_[unmask_ix]\n",
    "                    if self.quantize_cols_[unmask_ix]:\n",
    "                        pred_quant = top_p_sampling(cur_quant.n_bins_[0], pred_probas, rng, self.top_p)\n",
    "                        pred_val = cur_quant.inverse_transform_sample(pred_quant.reshape(1, 1))\n",
    "                    else:\n",
    "                        pred_quant = top_p_sampling(len(cur_quant.classes_), pred_probas, rng, self.top_p)\n",
    "                        pred_val = cur_quant.inverse_transform(pred_quant.reshape(1,))\n",
    "                    imputedX[kix, n, unmask_ix] = pred_val.item()\n",
    "        return imputedX\n",
    "\n",
    "rix = 0\n",
    "rng = check_random_state(rix)\n",
    "n_upper = 100\n",
    "n_lower = 100\n",
    "n = n_upper + n_lower\n",
    "data, labels = skd.make_moons(\n",
    "    (n_upper, n_lower), shuffle=False, noise=0.1, random_state=rix)\n",
    "data4impute = data.copy()\n",
    "data4impute[:, 1] = np.nan\n",
    "X=np.concatenate([data, data4impute], axis=0)\n",
    "\n",
    "model = MaskingTreesModel(n_bins=20)\n",
    "model.fit(X)\n",
    "data_fake = model.generate(n_samples=200);\n",
    "\n",
    "\n",
    "nimp = 1 # number of imputations needed\n",
    "data_impute = model.impute(k=nimp)[0, :, :]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(7, 5));\n",
    "axes[0, 0].scatter(data[:, 0], data[:, 1]);\n",
    "axes[0, 0].set_title('original');\n",
    "axes[0, 1].scatter(data_fake[:, 0], data_fake[:, 1]);\n",
    "axes[0, 1].set_title('generated');\n",
    "axes[1, 0].scatter(data_impute[200:, 0], data_impute[200:, 1]);\n",
    "axes[1, 0].set_title('imputed');\n",
    "\"\"\"\n",
    "axes[1, 1].scatter(data_impute[200:, 0], data_impute[200:, 1]);\n",
    "axes[1, 1].set_title('imputed - repainted');\n",
    "\"\"\"\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbcbea-a2db-4dc3-b39f-3ef4a107eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[\"cat_feature\"].astype(\"category\")\n",
    "# clf = xgb.XGBClassifier(tree_method=\"hist\", enable_categorical=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c973e-69eb-4ae3-81ca-a90f6e1e629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "X = X[list(X.select_dtypes(include=['number']).columns)]# + ['sex']]\n",
    "model = MaskingTreesModel(n_bins=5, duplicate_K=10)\n",
    "quantize_cols = [\n",
    "    col in list(X.select_dtypes(include=['floating']).columns)\n",
    "    for col in X.columns]\n",
    "model.fit(X.values, quantize_cols=quantize_cols)\n",
    "model.generate(n_samples=1)\n",
    "model.fit(X.values)\n",
    "model.generate(n_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79bd961-2918-433b-bf0a-83a2287e39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "my_data = load_iris()\n",
    "X, y = my_data['data'], my_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bb0d5-c4af-418b-a4e4-543b08a8fabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
